{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b8176-2302-445f-b98d-370f7fdc9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "from matplotlib.patches import Rectangle\n",
    "from lxml import etree\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# Enable Matplotlib backend for displaying static images\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2e2b0-37a5-4de5-82ca-3464b45c7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training history persistence & plotting utilities ---\n",
    "from pathlib import Path\n",
    "import json, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipdb\n",
    "\n",
    "# --- Run Config ---\n",
    "# Set to True to force retraining even if a saved .keras model exists.\n",
    "#FORCE_RETRAIN = True\n",
    "FORCE_RETRAIN = False\n",
    "\n",
    "# --- VGG16 ---\n",
    "VGG_MODEL_PATH = Path(\"data/model/vgg16/vgg16_model.keras\")\n",
    "VGG_MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "VGG_META_PATH  = VGG_MODEL_PATH.with_suffix(\".meta.json\")\n",
    "VGG_HISTORY_JSON = Path(\"data/model/vgg16/vgg16_history.json\")\n",
    "VGG_HISTORY_CSV  = VGG_HISTORY_JSON.with_suffix(\".csv\")\n",
    "\n",
    "# --- Xception ---\n",
    "XCP_MODEL_PATH = Path(\"data/model/xception/xception_model.keras\")\n",
    "XCP_MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "XCP_META_PATH  = XCP_MODEL_PATH.with_suffix(\".meta.json\")\n",
    "XCP_HISTORY_JSON = Path(\"data/model/xception/xception_history.json\")\n",
    "XCP_HISTORY_CSV  = XCP_HISTORY_JSON.with_suffix(\".csv\")\n",
    "\n",
    "# --- Inception ---\n",
    "ICP_MODEL_PATH = Path(\"data/model/inception/inception_model.keras\")\n",
    "ICP_MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "ICP_META_PATH  = ICP_MODEL_PATH.with_suffix(\".meta.json\")\n",
    "ICP_HISTORY_JSON = Path(\"data/model/inception/inception_history.json\")\n",
    "ICP_HISTORY_CSV  = ICP_HISTORY_JSON.with_suffix(\".csv\")\n",
    "\n",
    "# Config flags\n",
    "META_STRICT = False           # If True, require exact match on ALL keys\n",
    "CHECK_DATASET_SIG = False     # If True, enforce dataset signature equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccadde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_current_meta(model_arch: str, image_size):\n",
    "    \"\"\"\n",
    "    Build the CURRENT_META dict for metadata checks.\n",
    "    - model_arch: e.g. \"VGG16\", \"Xception\"\n",
    "    - image_size: e.g. (224, 224) or (299, 299)\n",
    "    \"\"\"\n",
    "    # Always refresh these live (in case folders changed)\n",
    "    class_names = get_class_names_from_dirs()\n",
    "    return {\n",
    "        \"image_size\": tuple(image_size),          # normalize to tuple\n",
    "        \"class_names\": class_names,\n",
    "        \"num_classes\": len(class_names),\n",
    "        \"preprocessing\": \"rescale(1/255)\",        # keep in sync with your pipeline\n",
    "        \"model_arch\": model_arch,\n",
    "        \"dataset_sig\": dataset_signature(),\n",
    "    }\n",
    "\n",
    "# Save the training history so we can reload it once training is done.\n",
    "def save_history(history, *, json_path=None, csv_path=None):\n",
    "    \"\"\"Persist Keras History to JSON and CSV for later reuse.\"\"\"\n",
    "    if history is None or not hasattr(history, \"history\"):\n",
    "        return\n",
    "    jp = Path(json_path) if json_path else HISTORY_JSON\n",
    "    cp = Path(csv_path)  if csv_path  else HISTORY_CSV\n",
    "    jp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # JSON\n",
    "    payload = {**history.history, \"epoch\": list(history.epoch)}\n",
    "    with open(jp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, indent=2, sort_keys=True)\n",
    "    # CSV\n",
    "    df = pd.DataFrame(payload)\n",
    "    df.to_csv(cp, index=False)\n",
    "\n",
    "# Return the training history as a Pandas DataFrame extracted from either a json or csv file.\n",
    "def load_history_df(*, json_path=None, csv_path=None):\n",
    "    \"\"\"Load previously saved history as a pandas DataFrame, if available.\"\"\"\n",
    "    jp = Path(json_path) if json_path else HISTORY_JSON\n",
    "    cp = Path(csv_path)  if csv_path  else HISTORY_CSV\n",
    "    if jp.exists():\n",
    "        with open(jp, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return pd.DataFrame(data)\n",
    "    if cp.exists():\n",
    "        return pd.read_csv(cp)\n",
    "    return None\n",
    "    \n",
    "# Plot the training history\n",
    "def plot_history_df(df):\n",
    "    \"\"\"Plot common metrics from a history DataFrame, if present.\"\"\"\n",
    "    if df is None or \"epoch\" not in df.columns:\n",
    "        print(\"No saved training history available to plot.\")\n",
    "        return\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    if \"loss\" in df.columns:\n",
    "        plt.plot(df[\"epoch\"], df[\"loss\"], label=\"train loss\")\n",
    "    if \"val_loss\" in df.columns:\n",
    "        plt.plot(df[\"epoch\"], df[\"val_loss\"], label=\"val loss\")\n",
    "    plt.legend()\n",
    "   \n",
    "    # Example: class accuracy if present\n",
    "    if \"val_class_acc\" in df.columns:\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Validation Accuracy (Class)\")\n",
    "        plt.plot(df[\"epoch\"], df[\"val_class_acc\"], label=\"val_class_acc\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # Example: MAEs if present\n",
    "    for col in [\"val_xmin_mae\",\"val_ymin_mae\",\"val_xmax_mae\",\"val_ymax_mae\"]:\n",
    "        if col not in df.columns:\n",
    "            return\n",
    "\n",
    "    plt.figure()                                      \n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Validation Mean Absolute Error (MAE)\")               \n",
    "    plt.plot(df[\"epoch\"], df[\"val_xmin_mae\"], label = \"xmin_MAE\")\n",
    "    plt.plot(df[\"epoch\"], df[\"val_ymin_mae\"], label = \"ymin_MAE\")\n",
    "    plt.plot(df[\"epoch\"], df[\"val_xmax_mae\"], label = \"xmax_MAE\")\n",
    "    plt.plot(df[\"epoch\"], df[\"val_ymax_mae\"], label = \"ymax_MAE\")\n",
    "    plt.legend() \n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4690b0-4064-4999-8a25-33c9c56f83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subfolders for the class labels\n",
    "os.listdir(\"data/images/images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382371c9-ea35-48c1-a482-0d615ceff99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotations of the class labels\n",
    "os.listdir(\"data/label/label\")[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0462eaab-9851-4d7b-b3c7-e03df6a396a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the images and count the number of image paths in the training subfolders\n",
    "image_path = glob.glob(\"data/images/images/*/*.jpg\")\n",
    "# Normalize slashes to one format first\n",
    "image_path = [p.replace('\\\\', '/') for p in image_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ade83f-0485-41cc-a317-362720880f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa5840-ece5-4572-8098-379884ef0b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the annotation files and count the number of annotations in the training label folder\n",
    "xmls_path = glob.glob(\"data/label/label/*.xml\")\n",
    "# Normalize slashes to one format first\n",
    "xmls_path = [p.replace('\\\\', '/') for p in xmls_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812e994b-3ff9-4280-9270-096105b581f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xmls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2079ea-e57f-4a29-b89c-23076af3675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls_path[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c9a7b-446a-4c66-b6f8-97b89290a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the annotation and image files in ascending order\n",
    "# The filenames are identical except for the extension.  This allows for a correct mapping of image to annotation by index.\n",
    "xmls_path.sort(key = lambda x:x.split(\"/\")[-1].split(\".xml\")[0])\n",
    "image_path.sort(key = lambda x:x.split(\"/\")[-1].split(\".jpg\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0337f6a-32bb-47ca-b8b9-4289ddf8f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls_path[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59c026-04c2-4254-bc69-a68c2ed71b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e60036-7c84-4236-92df-d871d9cbb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the annotation filename without extension\n",
    "xmls_train = [path.split(\"/\")[-1].split(\".\")[0] for path in xmls_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea9675-3a08-4020-a654-5b9a77de79f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmls_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06ef65-beb1-40e8-b808-fd4b26ca35bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the image filename that have a corresponding annotation file. Not all images have annotations (defects)\n",
    "# This will be the training set of images.\n",
    "imgs_train = [img for img in image_path if (img.split(\"/\")[-1].split)(\".jpg\")[0] in xmls_train]\n",
    "imgs_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697644f2-6197-4cd4-9336-57514eb8816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure equal counts of training images with the annotations\n",
    "len(imgs_train),len(xmls_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4f28b-3472-4efc-9219-c03c76a646af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# SET UP THE DATA FRAME #\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e7237-e54d-4f42-83db-d0ef9edd7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract class label from the path of each image file\n",
    "labels = [label.split(\"/\")[-2] for label in imgs_train]\n",
    "labels[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9df80-0ae6-4bf8-bed8-66a780cd522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels into a Pandas DataFrame for the class labels and call them Defect Types\n",
    "dataFrame = pd.DataFrame(labels, columns = [\"Defect Type\"])\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47b232-7e94-4650-a326-dd126c1c1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain training labels without duplication\n",
    "Class = dataFrame[\"Defect Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4fa112-1378-4ed2-9930-78ee005af010",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fe588-cd6e-4ed2-bd20-f5b89f3e6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair each element in Class with a corresponding number starting from 1\n",
    "zip_result = list(zip(Class, range(1, len(Class) + 1)))\n",
    "print(zip_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010f587-4e78-4c89-914c-c6ec2a50cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the tuple into a map of defect type to it's numerical reference (defect class index)\n",
    "Class_dict = dict(zip_result)\n",
    "Class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c6afe-fe01-44b8-96e1-e59ddee66269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called \"Class\" in the data frome and store the corresponding defect index\n",
    "dataFrame[\"Class\"] = dataFrame[\"Defect Type\"].apply(lambda x: Class_dict[x])\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c121c2-2f9a-4cd5-a28d-c6dd7cfd0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the class dictionary into binary vectors for easier processing.\n",
    "# Set up a One-hot encoding to represent the classification types.  A one-hot encoded vector is a way of representing categorical values as binary vectors\n",
    "# where exactly one element is 1 (the \"hot\" one) and all others are 0.  Each  is assigned a binary vector of 10 with the bit for the\n",
    "# specific defect type turned \"on\".\n",
    "lb = LabelBinarizer()\n",
    "# Fit it to the class dictionary\n",
    "lb.fit(list(Class_dict.values()))\n",
    "# Convert multi-class labels to binary labels (belong or does not belong) to the labels\n",
    "binary_labels = lb.transform(dataFrame[\"Class\"])\n",
    "binary_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70251aac-aa0a-4b48-9961-7d3f79af51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cb886-68d7-41d5-8114-407d249ef967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some new columns corresponding to the class index\n",
    "y_bin_labels = [] \n",
    "for i in range(binary_labels.shape[1]):\n",
    "    y_bin_labels.append(\"Class\" + str(i))\n",
    "    # Add the columns to the data frame.\n",
    "    dataFrame[\"Class\" + str(i + 1)] = binary_labels[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079eb22-1658-4af4-9e14-0a1f57fc6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c1c4d8-fbfb-402f-a028-e4f92a5622f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b89080-d79e-4ed6-8647-34f9d61f6e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above you see the english name of the classification plus both the associated numeric (Class) and binary classfications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18d9a9-e20f-4a94-91c8-ae1ee3b18989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the English and numerical classfication columns because the machine doesn't need them.\n",
    "dataFrame.drop(\"Class\", axis = 1, inplace = True)\n",
    "dataFrame.drop(\"Defect Type\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd897006-1b7b-4b85-949c-0efacd460560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the current data frame\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a99bd-9bbf-4c6d-872a-c72d84fe1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse and extract information from an annotation file and normalize bounding-box coordinates between 0,1\n",
    "# Relative bounding box coordinates express positions and dimensions as fractions of the image size, making them resolution-independent.\n",
    "def to_labels(path):\n",
    "    # Read the annotation file\n",
    "    xml = open(\"{}\".format(path)).read()                         \n",
    "    sel = etree.HTML(xml)\n",
    "    # Obtain the image width\n",
    "    width = int(sel.xpath(\"//size/width/text()\")[0])\n",
    "    # Obtain the image height\n",
    "    height = int(sel.xpath(\"//size/height/text()\")[0])  \n",
    "    # Extract the bounding box coordinates\n",
    "    xmin = int(sel.xpath(\"//bndbox/xmin/text()\")[0])\n",
    "    xmax = int(sel.xpath(\"//bndbox/xmax/text()\")[0])\n",
    "    ymin = int(sel.xpath(\"//bndbox/ymin/text()\")[0])\n",
    "    ymax = int(sel.xpath(\"//bndbox/ymax/text()\")[0])\n",
    "    # Return the relative coordinates\n",
    "    return [xmin/width, ymin/height, xmax/width, ymax/height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762166b-cede-4aa3-a4ef-b55d696106da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the relative bounding box coordinates for the annotations\n",
    "coords = [to_labels(path) for path in xmls_path]\n",
    "coords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f667dc0-e63e-495e-91a5-0ccc2926d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the coordinates and return lists for tuple element\n",
    "# Keep in mind that the lists are sorted the same as both xmls_path and image_path\n",
    "xmin, ymin, xmax, ymax = zip(*coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec536b6-12ea-4373-83c1-2da566eff5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin[:5], ymin[:5], xmax[:5], ymax[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f79b9-3688-4cf0-aa02-0159788caac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Numpy array.  Including the data frame values\n",
    "xmin = np.array(xmin)\n",
    "ymin = np.array(ymin)\n",
    "xmax = np.array(xmax)\n",
    "ymax = np.array(ymax)\n",
    "label_vector = np.array(dataFrame.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3116e4-d116-4d94-8d35-3bf5c057c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin[:5], ymin[:5], xmax[:5], ymax[:5], label_vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f4ce0-d6f9-40d1-abcc-9707223f8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a Dataset whose elements are slices of the given tensors\n",
    "# Slicing a 1D tensor produces scalar tensor elements\n",
    "\n",
    "# A tensor is a fundamental data structure in machine learning and deep learning — it's essentially \n",
    "# a multi-dimensional array that generalizes scalars, vectors, and matrices.\n",
    "\n",
    "# A tensor is like a NumPy ndarray or a general-purpose container for numbers.\n",
    "# You can think of it as a box that can hold numbers in any number of dimensions.\n",
    "\n",
    "# Sliced in parallel into a data set of 5 elements\n",
    "# (xmin, ymin, xmax, ymax, label_vector)\n",
    "# (0.84863281, 0.806, 0.95117188, 0.993, [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "# (0.85742188, 0.298, 0.96044922, 0.858, [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "# ...\n",
    "\n",
    "# The code builds a TensorFlow dataset where each element is a 5-tuple representing a bounding box and its label, \n",
    "# by slicing across the input sequences in parallel.\n",
    "\n",
    "labels_dataset = tf.data.Dataset.from_tensor_slices((xmin, ymin, xmax, ymax, label_vector))\n",
    "for img in labels_dataset.take(1):\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d24146-5c5f-4115-ac44-2e7f05441339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tensor Dataset is an iterator where...\n",
    "# Each element of the labels_dataset looks like this\n",
    "#   xmin, ymin, xmax, ymax, [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f11b9-3f78-4fb6-95e9-d60de9d81836",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c538c3-0766-4609-8ea5-06aad5c1f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a .jpg image from disk, resize it to 224x224 pixels, convert to format that TensorFlow can use from\n",
    "# training and normalize the pixel values to be between 0 and 1.\n",
    "def load_image(path):\n",
    "    # read raw bytes\n",
    "    image = tf.io.read_file(path)\n",
    "    # decode the image bytes into a usable image format and ensure it has 3 color channels (red, green, blue)\n",
    "    image = tf.image.decode_jpeg(image,3)       \n",
    "    # resize to 224x224 - common size  used in many pretrained models\n",
    "    image = tf.image.resize(image,[224,224])\n",
    "    # converts the pixel values to 32-bit floating point numbers (from integers).  Neural networks typically expect inputs\n",
    "    # as floating-point numbers also, when dividing by 255 we don't to truncate to 0 or 1.\n",
    "    image = tf.cast(image,tf.float32)  \n",
    "    image = image / 255\n",
    "    # normalize pixel values from [0-255] to [0.0-1.0]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c7a46-d2ff-4d33-8178-c6bc125b8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8ef6b-8acb-45df-b5d7-38ad6ae078ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For feeding images into a convolutional neural network (CNN) for classification or detection\n",
    "# create a dataset where each item is an image tensor of shape (224, 224, 3) — a color image — and the pixel values are of type float32.\n",
    "# Takes a list of image file paths and creates a dataset where each elelment is a file path string\n",
    "img_dataset = tf.data.Dataset.from_tensor_slices(imgs_train)\n",
    "img_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ead42-cb88-46cf-8088-9524892b252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in img_dataset.take(1):\n",
    "    print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31bb4dd-c8d6-4578-8f9c-7f7ef6480c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element of the img_dataset looks like this\n",
    "#   jpg image filepath string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a784743-9a24-4cdd-ae4a-e11129e07c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map (apply) the load_image function to img_dataset\n",
    "# each element in the img_dataset is a preprocessed image tensor (224 x 224 pixels with 3 color channels)\n",
    "img_dataset = img_dataset.map(load_image)\n",
    "img_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122d481-4a38-4ac3-94f3-ec56f38c43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element of the img_dataset now looks like this\n",
    "#   [224 x 224 x 3] array of pixels as float32 numbers\n",
    "#   224 columns X 224 rows, where each cell is a normalized array of RGB intensities [0.8, 0.2, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f36fd-8b0b-42fd-b751-41ca5d69caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two separate TensorFlow datasets — dataset (e.g. images) and labels_dataset (e.g. bounding boxes or class labels)\n",
    "# into a single dataset where each element is a pair: (image, label).\n",
    "dataset_label = tf.data.Dataset.zip((img_dataset, labels_dataset))\n",
    "dataset_label.element_spec\n",
    "#for img in dataset_label.take(1):\n",
    "    #print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70489f5e-495d-4f3f-b023-ccbf22191e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element of the dataset_label looks like this\n",
    "# 1. The image:\n",
    "#   [224 x 224 x 3] array of pixels as float32 numbers\n",
    "# 2. Where is the defect and what kind of defect.\n",
    "#   xmin, ymin, xmax, ymax, [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69424841-71ab-44f0-a243-723cb10d8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures the TensorFlow dataset pipeline for efficient training by applying several transformations in sequence.\n",
    "# repeat() = repeats the dataset indefinitely until you stop it (e.g., by setting steps_per_epoch in model.fit()).\n",
    "#    Without it, the dataset would run out after one pass.\n",
    "# shuffle(500) = Randomly shuffles the order of elements in the dataset.\n",
    "#    The argument 500 is the size of the shuffle buffer: TensorFlow maintains a buffer of 500 elements and randomly draws from it.\n",
    "# batch(batch_size) = Groups the data into batches of 32 samples. Instead of yielding one (image, label) pair at a time, \n",
    "#    the dataset now yields (batch_of_images, batch_of_labels).\n",
    "batch_size = 32\n",
    "dataset_label = dataset_label.repeat().shuffle(500).batch(batch_size)\n",
    "dataset_label.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5cb1cd-462a-4986-8f8f-2fa0e97e5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into 80% for training and 20% for testing\n",
    "train_count = int(len(imgs_train) * 0.8)\n",
    "test_count = int(len(imgs_train) * 0.2)\n",
    "train_count, test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670fe88-5c7e-4aa2-b50a-ce15bf07b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff0e5e-7956-4e1c-b8c0-e54d8cf7ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_label.skip(test_count)\n",
    "test_dataset = dataset_label.take(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486f5c3-8e4a-4f50-b85c-60d2e6ba42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d3788-30f3-42c2-8264-9508ecd09238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip the class dictionary\n",
    "class_dict = {v:k for k,v in Class_dict.items()}\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0795612-ccfa-4cbc-8774-b1ff74590060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample training image with its label\n",
    "# Takes one batch from the train_dataset (which yields batches of images and labels).\n",
    "for img, label in train_dataset.take(1):\n",
    "    # show the first image in the batch. Convert the float pixel values in to PIL image that can be plotted\n",
    "    plt.imshow(keras.preprocessing.image.array_to_img(img[0]))\n",
    "    # Unpacks the label into its components (xmin tensor, ymin tensor, xmax tensor, ymax tensor, class one hot tensor\n",
    "    out1, out2, out3, out4, out5 = label\n",
    "    # Each outX is a batch of images so grap the first image and de-normalize the value from [0,1] back to [0,224]\n",
    "    xmin, ymin, xmax, ymax = out1[0].numpy()*224, out2[0].numpy()*224, out3[0].numpy()*224, out4[0].numpy()*224\n",
    "    # Creates a red rectangle object (bounding box) using matplotlib's Rectangle. \n",
    "    # The box is positioned at (xmin, ymin) with width/height derived from the bounding box coordinates.\n",
    "    rect = Rectangle((xmin,ymin),(xmax - xmin),(ymax - ymin), fill = False, color = \"r\")\n",
    "    # Gets the current axes object from matplotlib — needed to add custom shapes like rectangles.\n",
    "    ax = plt.gca()\n",
    "    # Adds the bounding box rectangle to the current image plot.\n",
    "    ax.axes.add_patch(rect)\n",
    "    # Creates a list to hold the predicted class label string for the image (though you're only adding one item here).\n",
    "    pred_imglist = []\n",
    "    # np.argmax(out5[0]) finds the index of the 1 in the one-hot vector (e.g., 2 if the class is 3).\n",
    "    # +1 assumes your class labels start at 1 rather than 0.\n",
    "    # class_dict[...] maps that numeric class ID to a human-readable string like \"dent\" or \"scratch\".\n",
    "    pred_imglist.append(class_dict[np.argmax(out5[0])+1])\n",
    "    plt.title(pred_imglist)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d2f9a-8b7e-4717-9cc0-ad55d1582014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import VGG16 model with pre-trained weights from ImageNet\n",
    "# loads the VGG16 neural network, pre-trained on the ImageNet dataset, without its final classification layer (top), \n",
    "# and sets it up to accept images of shape 224×224 with 3 color channels (RGB).\n",
    "# pooling='avg'. This turns the final convolutional feature maps into a single feature vector by averaging across the spatial dimensions.\n",
    "# Output is a flat vector instead of a 3D feature map — which makes it easier to connect to a custom classification head later.\n",
    "# This way, you can train your own classifier on top of VGG16’s powerful features, even with a smaller dataset.\n",
    "#\n",
    "# Summary: This code loads a pre-trained VGG16 model without the final classification layer, ready to be used as a feature extractor \n",
    "# for your own image classification or object detection task.\n",
    "#base_vgg16 = tf.keras.applications.VGG16(weights = \"imagenet\", include_top = False, input_shape = (224,224,3), pooling = 'avg')\n",
    "\n",
    "# Big Picture: What is this model doing?\n",
    "# Imagine a robot with eyes (VGG16). You show it an image (e.g., of a metal part), and it:\n",
    "# Figures out where a defect is — like drawing a red box around it (xmin, ymin, xmax, ymax)\n",
    "# Decides what kind of defect it is — like saying “scratch” or “dent” (classification)\n",
    "# So this model is doing two tasks at once:\n",
    "# Object localization (bounding box regression)\n",
    "# Object classification (what kind of thing is in the box)\n",
    "# It's like a 3-part assembly line:\n",
    "# [Image] → [Feature Extractor] → [Two Output Heads: Bounding Box + Class]\n",
    "\n",
    "base_vgg16 = tf.keras.applications.VGG16(weights = 'data/misc/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top = False, input_shape = (224,224,3), pooling = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f95595-8372-486a-8ac8-2e796d0722ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The robot is trainiable\n",
    "base_vgg16.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b3033-0ae8-4902-95c5-4c3ae9378dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dear robot, expect each input to look like an image that’s 224x224 pixels with 3 color channels (RGB).\n",
    "# (height, width, channels) = (224, 224, 3)\n",
    "# In Keras Functional API, each time you pass a tensor through a layer, you're chaining a step in the computation graph. \n",
    "# That flow determines the model hierarchy.\n",
    "# - input_layer_1\n",
    "inputs = keras.Input(shape = (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81c34aa-5d3c-4b76-9130-23a48675c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be a 1D feature vector — a kind of \"summary\" of what the model sees in the image.\n",
    "# Robot, When a real image of shape (224, 224, 3) comes in, this is how it should flow through VGG16 to produce a 512-dimensional output.\n",
    "# We're building a flowchart here (a graph of how data should move) not doing any real compution yet.\n",
    "# - input_layer_1 --> vgg16\n",
    "x = base_vgg16(inputs)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca996f-1950-466d-99de-1e4d6fba2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head #1: \n",
    "# Bounding Box (Where is the object?)\n",
    "# Think of this as a machine that processes features into spatial understanding. \n",
    "# It transforms abstract visual features into more concrete location information.\n",
    "# - input_layer_1 --> vgg16 --> dense (optionally name=\"dense\")\n",
    "# Create a layer object and pass the output of vgg16 (x) to it and return a new tensor value (x1)\n",
    "x1 = keras.layers.Dense(1024, activation = \"relu\")(x) #dense\n",
    "# - input_layer_1 --> vgg16 --> dense_1\n",
    "# feed x1 into another dense layer a with 512 units and override x1 with the new output\n",
    "x1 = keras.layers.Dense(512, activation = \"relu\")(x1) #dense_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b3a01-745d-498b-b655-a0e8870d160a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The machine outputs four floating-point numbers, one for each corner of the bounding box — as if drawing a box around a defect.\n",
    "# Where is the defect\n",
    "# - input_layer_1 --> vgg16 --> dense_1 --> xmin\n",
    "out1 = keras.layers.Dense(1, name = \"xmin\")(x1)\n",
    "# - input_layer_1 --> vgg16 --> dense_1 --> ymin\n",
    "out2 = keras.layers.Dense(1, name = \"ymin\")(x1)\n",
    "# - input_layer_1 --> vgg16 --> dense_1 --> xmax\n",
    "out3 = keras.layers.Dense(1, name = \"xmax\")(x1)\n",
    "# - input_layer_1 --> vgg16 --> dense_1 --> ymax\n",
    "out4 = keras.layers.Dense(1, name = \"ymax\")(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a076d4a-7c5c-4abd-a338-c5199baecb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head #2: \n",
    "# Classification (What is the defect?)\n",
    "# This second machine takes the same visual input but tries to decide what it's looking at. \n",
    "# The Dropout is like forcing the model to generalize by hiding parts of what it sees — to avoid overconfidence.\n",
    "# - input_layer_1 --> vgg16 --> dense_2\n",
    "x2 = keras.layers.Dense(1024,activation = \"relu\")(x) #dense_2\n",
    "x2 = keras.layers.Dropout(0.5)(x2)\n",
    "x2 = keras.layers.Dense(512,activation = \"relu\")(x2) #dense_3\n",
    "# This gives you a probability distribution over 10 classes — like saying: “There’s a 70% chance it’s a scratch, 20% it’s a dent, 10% inclusion.”\n",
    "out_class = keras.layers.Dense(10,activation = \"softmax\", name = \"class\")(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73267128-23c4-4bf5-99d0-0cb754f34636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both outputs:\n",
    "# You’re producing 5 outputs: 4 numbers for the bounding box 1 vector (length 10) for the class prediction\n",
    "#   4 bounding box values: xmin, ymin, xmax, ymax → shape (None, 1)\n",
    "#   1 classification output: class → shape (None, 10)\n",
    "out = [out1, out2, out3, out4, out_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a274c69-a290-49e6-8174-b4e9f446cef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This creates the full model and shows a summary of layers, shapes, and parameters.\n",
    "vgg16 = keras.models.Model(inputs = inputs, outputs = out)\n",
    "vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b12a6-114c-48a5-9822-07201d71efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer: a single computational step in the model. It takes input data, performs some transformation, and passes the result to the next layer\n",
    "#   input_layer_1: Placeholder for input image\n",
    "#   vgg16: The pre-trained VGG16 feature extractor\n",
    "#   dense_2: Fully connected layer with 1024 units\n",
    "#   dropout: Randomly drops 50% of inputs during training\n",
    "#   x/ymin, x/ymax, class: Final output heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aeee36-bbce-4994-ad0a-26947cc80ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#                            +--------------------------+\n",
    "#                            | input_layer_1 (Input)    |\n",
    "#                            | shape: (224, 224, 3)     |\n",
    "#                            +------------+-------------+\n",
    "#                                         |\n",
    "#                            +--------------------------+\n",
    "#                            | vgg16 (Functional)       |\n",
    "#                            | output shape: (512,)     |\n",
    "#                            +------------+-------------+\n",
    "#                                         |\n",
    "#                    +----Bounding Box----+----Classification---+\n",
    "#                    |                                          |\n",
    "#                    x                                          x\n",
    "#         +---------------------+                     +---------------------+\n",
    "#         | dense (1024, relu)  |                     | dense_2 (1024, relu)|\n",
    "#         +----------+----------+                     +----------+----------+\n",
    "#                    |                                           |\n",
    "#                    x1                                          x2\n",
    "#         +---------------------+                     +---------------------+\n",
    "#         | dense_1 (512, relu) |                     | dropout (rate=0.5)  |\n",
    "#         +----------+----------+                     +----------+----------+\n",
    "#                    |                                           |\n",
    "#                    x1                                          x2\n",
    "#         +-------+------+--------+                   +----------------------+\n",
    "#         |       |       |       |                   | dense_3 (512, relu)  |\n",
    "#         |       |       |       |                   +----------+-----------+\n",
    "#         |       |       |       |                              |\n",
    "#         x1      x1      x1      x1                             x2\n",
    "#     +------+ +------+ +------+ +------+             +------------------------+\n",
    "#     | xmin | | ymin | | xmax | | ymax |             | class (Dense, softmax) |\n",
    "#     | (1)  | | (1)  | | (1)  | | (1)  |             | shape: (None, 10)      |\n",
    "#     +------+ +------+ +------+ +------+             +-----------+------------+\n",
    "#                                                                 |\n",
    "#                                                                 x2\n",
    "#                                                     +------------------------+\n",
    "#                                                     |   10-class prediction  |\n",
    "#                                                     +------------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a9b10-d0fe-47f0-8138-964f022c6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with optimizer and loss functions\n",
    "# This code prepares your model for training by telling Keras:\n",
    "# How to adjust the model’s weights (the optimizer)\n",
    "# How to measure errors (the loss functions)\n",
    "# What metrics to track during training (like accuracy and mean absolute error)\n",
    "# Uses the Adam optimizer, a popular and efficient way of updating weights during training.\n",
    "# The learning rate is set to 0.0005 — a relatively small step size for stable learning.\n",
    "# loss = {\n",
    "#        \"xmin\": \"mse\",\n",
    "#        \"ymin\": \"mse\",\n",
    "#        \"xmax\": \"mse\",\n",
    "#        \"ymax\": \"mse\",\n",
    "#        \"class\": \"categorical_crossentropy\"\n",
    "# }\n",
    "# This model has five outputs: xmin, ymin, xmax, ymax, and class\n",
    "# Use Mean Squared Error (MSE) for the 4 bounding box outputs.  Good for continuous values like coordinates.\n",
    "# Use Categorical Crossentropy for the class output. Standard for multi-class classification with one-hot encoded labels.\n",
    "# metrics = [\"mae\", \"acc\"] \n",
    "#  \"mae\" = Mean Absolute Error (good for bounding boxes)\n",
    "#  \"acc\" = Accuracy (used on the class output)\n",
    "#######\n",
    "#  “Hey Keras — use the Adam optimizer with a small learning rate. For the four bounding box outputs, \n",
    "#   use MSE to measure how far off the predictions are. For the class output, use categorical crossentropy. \n",
    "#   While training, also track how far off the boxes are on average (mae) and how often the predicted class is correct (acc).”\n",
    "#######\n",
    "vgg16.compile(keras.optimizers.Adam(0.0005),\n",
    "   loss = { \n",
    "      \"xmin\": \"mse\",\n",
    "      \"ymin\": \"mse\",\n",
    "      \"xmax\": \"mse\",\n",
    "      \"ymax\": \"mse\",\n",
    "      \"class\": \"categorical_crossentropy\" \n",
    "   },\n",
    "   metrics = {\n",
    "      \"xmin\": \"mae\",\n",
    "      \"ymin\": \"mae\",\n",
    "      \"xmax\": \"mae\",\n",
    "      \"ymax\": \"mae\",\n",
    "      \"class\": \"acc\"\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595f56d-2ea9-4971-9bd2-d8c6bbf72c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce learning rate\n",
    "# Creates a training callback that automatically adjusts (reduces) the learning rate if your model’s performance stops improving.\n",
    "#    \"val_loss\"  : Monitors the validation loss (error on your validation set)\n",
    "#    patience=5\t : Waits 5 epochs without improvement before taking action\n",
    "#    factor=0.5\t : Reduces the learning rate by multiplying it by 0.5 (i.e., halves it)\n",
    "#    min_lr=1e-6 : Ensures the learning rate never drops below 0.000001\n",
    "lr_reduce = keras.callbacks.ReduceLROnPlateau(\"val_loss\", patience = 5, factor = 0.5, min_lr = 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acb8be5-29fd-49ad-a32d-9de4145d9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below cell is primarily used for saving and loading meta data.  This allows us to train a model and save it\n",
    "# so that we can run the script again and not have to go through the training which takes days.\n",
    "# It's not enough to just save the keras data file. We save a bunch of associated meta data to help ensure that\n",
    "# Nothing has changed since we last trained the model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model/dataset metadata to ensure safe reuse of saved model ---\n",
    "from pathlib import Path\n",
    "import json, hashlib, os, glob\n",
    "\n",
    "# Returns the names of the defect directories (crease, oil_spot, etc.)\n",
    "def get_class_names_from_dirs(root_dir=\"data/images/images\"):\n",
    "    # Assumes GC10-DET is arranged as data/images/images/<class>/*.jpg (or png)\n",
    "    if not os.path.isdir(root_dir):\n",
    "        return []\n",
    "    names = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "    names.sort()\n",
    "    return names\n",
    "\n",
    "# Creates a hash of the class name and number of instances (crease:52) so that we can verify none\n",
    "# of the image files were deleted between training and reloading history.\n",
    "def dataset_signature(root_dir=\"data/images/images\"):\n",
    "    # Produce a quick signature of dataset contents (class names + file counts)\n",
    "    if not os.path.isdir(root_dir):\n",
    "        return \"missing\"\n",
    "    parts = []\n",
    "    for cls in sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]):\n",
    "        files = glob.glob(os.path.join(root_dir, cls, \"*\"))\n",
    "        parts.append(f\"{cls}:{len(files)}\")\n",
    "    digest = hashlib.sha1(\"|\".join(parts).encode(\"utf-8\")).hexdigest()\n",
    "   \n",
    "    return digest\n",
    "\n",
    "def load_meta(path):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def diff_meta(saved: dict, current: dict):\n",
    "    \"\"\"Return {key: (saved, current)} for keys that differ.\"\"\"\n",
    "    diffs, all_keys = {}, set((saved or {}).keys()) | set((current or {}).keys())\n",
    "    for k in sorted(all_keys):\n",
    "        if (saved or {}).get(k) != (current or {}).get(k):\n",
    "            diffs[k] = ((saved or {}).get(k), (current or {}).get(k))\n",
    "    return diffs\n",
    "\n",
    "# --- Canonicalization helpers for robust meta comparison ---\n",
    "def _norm_size(v):\n",
    "    try:\n",
    "        a, b = v\n",
    "        return (int(a), int(b))  # always compare as a tuple of ints\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _norm_class_names(v):\n",
    "    try:\n",
    "        return sorted(list(v))   # compare as a sorted list\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def _canon_meta(meta: dict, *, strict: bool, check_sig: bool):\n",
    "    if meta is None:\n",
    "        return None\n",
    "    # Base keys always enforced\n",
    "    keys = [\"image_size\", \"num_classes\", \"class_names\", \"model_arch\"]\n",
    "    # Strict mode can include additional invariants you care about\n",
    "    if strict:\n",
    "        keys += [\"preprocessing\"]  # add more if you want iron-clad parity\n",
    "    if check_sig:\n",
    "        keys += [\"dataset_sig\"]\n",
    "\n",
    "    canon = {}\n",
    "    for k in keys:\n",
    "        v = meta.get(k)\n",
    "\n",
    "        if k == \"image_size\":\n",
    "            v = _norm_size(v)\n",
    "        elif k == \"class_names\":\n",
    "            v = _norm_class_names(v)\n",
    "        # everything else compared as-is\n",
    "        canon[k] = v\n",
    "    return canon\n",
    "\n",
    "def meta_matches(saved: dict, current: dict) -> bool:\n",
    "    if saved is None or current is None:\n",
    "        return False\n",
    "    s = _canon_meta(saved,   strict=META_STRICT, check_sig=CHECK_DATASET_SIG)\n",
    "    c = _canon_meta(current, strict=META_STRICT, check_sig=CHECK_DATASET_SIG)\n",
    "    return s == c\n",
    "\n",
    "# (Optional) make saved JSON consistent going forward\n",
    "def save_meta(meta: dict, path: Path):\n",
    "    meta = dict(meta)\n",
    "    # store image_size canonically as a list for JSON readability\n",
    "    if isinstance(meta.get(\"image_size\"), tuple):\n",
    "        meta[\"image_size\"] = list(meta[\"image_size\"])\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2, sort_keys=True)\n",
    "\n",
    "# Derive current meta from filesystem & notebook assumptions\n",
    "CURRENT_META = {\n",
    "    \"image_size\": (224, 224),             # VGG16 input size (adjust if you changed it elsewhere)\n",
    "    \"class_names\": get_class_names_from_dirs(),\n",
    "    \"num_classes\": len(get_class_names_from_dirs()),\n",
    "    \"preprocessing\": \"rescale(1/255)\",    # keep in sync with your pipeline\n",
    "    \"model_arch\": \"VGG16\",\n",
    "    \"dataset_sig\": dataset_signature(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499143f-c124-4209-ba4a-0bceee867a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# This function runs the training loop for your custom VGG16-based model in Keras. \n",
    "# It's where your model finally sees the data and learns from it.\n",
    "#     Training data: train_dataset\n",
    "#     Validation data: test_dataset\n",
    "#######\n",
    "# This code trains the model for up to 200 epochs, using batches of data from train_dataset. \n",
    "# After each epoch, it evaluates on test_dataset. If validation loss stops improving for 5 epochs, \n",
    "# it reduces the learning rate to help refine learning. All training stats are stored in the history object.\n",
    "# Returns History: keras.callbacks.History\n",
    "#     history.history → a dict mapping metric names to lists of values, one entry per epoch.\n",
    "#        Example: 'loss': [...], 'val_loss': [...], 'accuracy': [...], 'val_accuracy': [...], ...}\n",
    "#     history.epoch → a list of epoch indices that actually ran.\n",
    "#        Example: [0, 1, 2, ..., 199]\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3cfe1-6d6a-460a-934b-2aedd33b96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    test_dataset,\n",
    "    train_count: int,\n",
    "    test_count: int,\n",
    "    batch_size: int,\n",
    "    *,\n",
    "    epochs: int = 200,\n",
    "    callbacks=None,\n",
    "    verbose: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a Keras model with standard steps/epoch math and return the History.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras.Model\n",
    "        Your compiled model (e.g., vgg16).\n",
    "    train_dataset, test_dataset : tf.data.Dataset\n",
    "        Datasets yielding (x, y) batches.\n",
    "    train_count, test_count : int\n",
    "        Number of samples in train/validation sets.\n",
    "    batch_size : int\n",
    "        Batch size used to compute steps.\n",
    "    epochs : int, default 200\n",
    "        Max epochs.\n",
    "    callbacks : list, optional\n",
    "        Keras callbacks (e.g., [lr_reduce]).\n",
    "    verbose : int, default 1\n",
    "        Keras fit verbosity.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    history : tf.keras.callbacks.History\n",
    "    \"\"\"\n",
    "    if callbacks is None:\n",
    "        callbacks = []\n",
    "\n",
    "    steps_per_epoch = max(1, train_count // batch_size)\n",
    "    validation_steps = max(1, test_count // batch_size)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs,\n",
    "        validation_data=test_dataset,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7485a-f310-4d49-822f-5101d813c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "MODEL_PATH = VGG_MODEL_PATH\n",
    "META_PATH  = MODEL_PATH.with_suffix(\".meta.json\")\n",
    "CURRENT_META = make_current_meta(\"VGG16\", (224, 224))\n",
    "saved_meta = load_meta(VGG_META_PATH)\n",
    "can_reuse = MODEL_PATH.exists() and meta_matches(saved_meta, CURRENT_META) and not FORCE_RETRAIN\n",
    "\n",
    "if can_reuse:\n",
    "    print(f\"Metadata matched. Loading model from {MODEL_PATH} (compile=True).\")\n",
    "    vgg16 = keras.models.load_model(MODEL_PATH, compile=True)\n",
    "else:\n",
    "    if MODEL_PATH.exists() and not meta_matches(saved_meta, CURRENT_META):\n",
    "        diffs = diff_meta(saved_meta, CURRENT_META)\n",
    "        print(\"Saved model metadata does not match current dataset/config. Differences:\")\n",
    "        for k, (a, b) in diffs.items():\n",
    "            print(f\" - {k}: saved={a} current={b}\")\n",
    "    elif MODEL_PATH.exists() and FORCE_RETRAIN:\n",
    "        print(\"FORCE_RETRAIN=True: retraining despite existing saved model...\")\n",
    "    else:\n",
    "        print(\"No saved model found. Training from scratch...\")\n",
    "    # Train, then save model + metadata (and history if the helper exists)\n",
    "    history_obj = train_model(\n",
    "        vgg16,\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        train_count=train_count,\n",
    "        test_count=test_count,\n",
    "        batch_size=batch_size,\n",
    "        epochs=200,\n",
    "        callbacks=[lr_reduce],   # keep your callbacks\n",
    "    )\n",
    "    vgg16.save(str(MODEL_PATH), include_optimizer=False)\n",
    "    save_meta(CURRENT_META, META_PATH)\n",
    "    save_history(\n",
    "        history_obj,\n",
    "        json_path=str(VGG_HISTORY_JSON),\n",
    "        csv_path=str(VGG_HISTORY_CSV),\n",
    "    )\n",
    "    print(f\"Model saved to {MODEL_PATH} and metadata to {META_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05985cdd-82d6-4f09-91a2-32a1c981936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is where we actually train the model but only if FORCE_RETRAIN is True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1393c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot previously saved training history (works even when skipping training) ---\n",
    "# -- hist_df is a Pandas DataFrame\n",
    "hist_df = load_history_df(\n",
    "    json_path=str(VGG_HISTORY_JSON),\n",
    "    csv_path=str(VGG_HISTORY_CSV),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee959f23-f210-4f75-99e7-ff1b95b565ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Save moved into the load-or-train cell to ensure single save point.\n",
    "# Original code commented out to avoid duplicate saves.\n",
    "# vgg16.save(\"data/model/vgg16_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ddade-e1cb-45be-aee4-37c3a032b4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hist_df is not None:\n",
    "    plot_history_df(hist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d430de-75ef-495e-ae78-d31458896217",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vgg16.evaluate(test_dataset, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab6eb65-0c91-4d4b-8403-74c97384cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE of xmin value in test set:{}\".format(results['xmin_mae']))\n",
    "print(\"MAE of ymin value in test set:{}\".format(results['ymin_mae']))\n",
    "print(\"MAE of xmax value in test set:{}\".format(results['xmax_mae']))\n",
    "print(\"MAE of ymax value in test set:{}\".format(results['ymax_mae']))\n",
    "print(\"Testing accuracy of predicted label:{}\".format(results['class_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e9420-b1e4-43e1-ae19-d2e1a2a0e2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 24))\n",
    "for img, _ in test_dataset.take(1):\n",
    "    out1, out2, out3, out4, label = vgg16.predict(img)\n",
    "    for i in range(3):\n",
    "        plt.subplot(3, 1, i + 1)            \n",
    "        plt.imshow(keras.preprocessing.image.array_to_img(img[i]))    \n",
    "        pred_imglist = []\n",
    "        pred_imglist.append(class_dict[np.argmax(out5[i]) + 1])\n",
    "        plt.title(pred_imglist)\n",
    "        xmin, ymin, xmax, ymax = out1[i]*224, out2[i]*224, out3[i]*224, out4[i]*224\n",
    "        rect = Rectangle((xmin,ymin), (xmax - xmin), (ymax - ymin), fill = False, color = \"r\") \n",
    "        ax = plt.gca()                   \n",
    "        ax.axes.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb1a2f-4337-4b9a-ad16-69972fa9c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the true labels into a list\n",
    "true_labels = []\n",
    "for _, label in test_dataset:    \n",
    "    out1, out2, out3, out4, out5 = label                              \n",
    "    true_labels.append(class_dict[np.argmax(out5) + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c242f1d-de15-4fb4-b509-406150ce7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d300bac-ad97-4ffe-8565-c907f696115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "for img, label in test_dataset:\n",
    "    out1, out2, out3, out4, out5 = label\n",
    "    label = vgg16.predict(img)\n",
    "    test_labels.append(class_dict[np.argmax(out5) + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579e3c02-e91f-405f-bc3d-167ada2dd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0767668-4cec-475c-be89-bb1c989f0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['cresent_gap', 'crease', 'silk_spot', 'water_spot', 'welding_line', \n",
    "                'inclusion', 'oil_spot', 'waist_folding', 'rolled_pit', 'punching_hole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f24fd-f80e-4504-a4ad-48a0d27ba880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels, test_labels, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413493e8-af44-4b59-861e-c17c5bf88f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(true_labels, test_labels, labels = target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e211cd2c-8b59-4820-895b-124c6b6e18cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_xception = tf.keras.applications.Xception(weights = \"imagenet\",\n",
    "                                               include_top = False,\n",
    "                                               input_shape = (224,224,3),\n",
    "                                               pooling = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9067eb8-c4a4-45e3-80f9-c6c882ccd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the base model with fully-connected layers\n",
    "base_xception.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df10df-d310-4dcd-9386-553937123d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model with fully-connected layers\n",
    "inputs = keras.Input(shape = (224,224,3))\n",
    "x = base_xception(inputs)\n",
    "\n",
    "x1 = keras.layers.Dense(1024, activation = \"relu\")(x)\n",
    "x1 = keras.layers.Dense(512, activation = \"relu\")(x1)\n",
    "out1 = keras.layers.Dense(1, name = \"xmin\")(x1)\n",
    "out2 = keras.layers.Dense(1, name = \"ymin\")(x1)\n",
    "out3 = keras.layers.Dense(1, name = \"xmax\")(x1)\n",
    "out4 = keras.layers.Dense(1, name = \"ymax\")(x1)\n",
    "\n",
    "x2 = keras.layers.Dense(1024, activation = \"relu\")(x)\n",
    "x2 = keras.layers.Dropout(0.5)(x2)\n",
    "x2 = keras.layers.Dense(512, activation = \"relu\")(x2)\n",
    "out_class = keras.layers.Dense(10, activation = \"softmax\", name = \"class\")(x2)\n",
    "\n",
    "out = [out1, out2, out3, out4, out_class]\n",
    "\n",
    "xception = keras.models.Model(inputs = inputs, outputs = out)\n",
    "xception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75609e11-13a9-4887-bb8d-456ad56d90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with optimizer and loss functions\n",
    "\n",
    "xception.compile(keras.optimizers.Adam(0.0005),\n",
    "   loss = { \n",
    "      \"xmin\" : \"mse\",\n",
    "      \"ymin\" : \"mse\",\n",
    "      \"xmax\" : \"mse\",\n",
    "      \"ymax\" : \"mse\",\n",
    "      \"class\": \"categorical_crossentropy\" \n",
    "   },\n",
    "   metrics = {\n",
    "      \"xmin\" : \"mae\",\n",
    "      \"ymin\" : \"mae\",\n",
    "      \"xmax\" : \"mae\",\n",
    "      \"ymax\" : \"mae\",\n",
    "      \"class\": \"acc\"\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209efb66-3782-4bd6-bc77-a3ec40ffd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = XCP_MODEL_PATH\n",
    "CURRENT_META = make_current_meta(\"Xception\", (224, 224))\n",
    "META_PATH  = MODEL_PATH.with_suffix(\".meta.json\")\n",
    "saved_meta = load_meta(XCP_META_PATH)\n",
    "can_reuse = MODEL_PATH.exists() and meta_matches(saved_meta, CURRENT_META) and not FORCE_RETRAIN\n",
    "\n",
    "if can_reuse:\n",
    "    print(f\"Loading Xception from {MODEL_PATH} (compile=True).\")\n",
    "    xception = keras.models.load_model(MODEL_PATH, compile=True)\n",
    "    # Load last training curves for plotting\n",
    "    hist_df = load_history_df(\n",
    "        json_path=str(XCP_HISTORY_JSON),\n",
    "        csv_path=str(XCP_HISTORY_CSV),\n",
    "    )\n",
    "else:\n",
    "    print(\"Training Xception...\")\n",
    "    history = train_model(\n",
    "        xception,\n",
    "        train_dataset, test_dataset,\n",
    "        train_count=train_count, test_count=test_count,\n",
    "        batch_size=batch_size, epochs=200,\n",
    "        callbacks=[lr_reduce],\n",
    "    )\n",
    "    xception.save(str(MODEL_PATH), include_optimizer=False)\n",
    "    save_meta(CURRENT_META, META_PATH)\n",
    "\n",
    "    # Persist curves next to the model\n",
    "    save_history(\n",
    "        history,\n",
    "        json_path=str(XCP_HISTORY_JSON),\n",
    "        csv_path=str(XCP_HISTORY_CSV),\n",
    "    )\n",
    "    # handy for immediate plotting\n",
    "    hist_df = pd.DataFrame({**history.history, \"epoch\": list(history.epoch)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb3333a-cc94-46bf-a02e-dc438143c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_df(hist_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c040c67-944d-4503-937e-a53c6fe42672",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = xception.evaluate(test_dataset, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037b4a7-fb76-4f3d-b48d-566d42a76694",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE of xmin value in test set:{}\".format(results['xmin_mae']))\n",
    "print(\"MAE of ymin value in test set:{}\".format(results['ymin_mae']))\n",
    "print(\"MAE of xmax value in test set:{}\".format(results['xmax_mae']))\n",
    "print(\"MAE of ymax value in test set:{}\".format(results['ymax_mae']))\n",
    "print(\"Testing accuracy of predicted label:{}\".format(results['class_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0972ef56-c87d-4dbc-8ebe-d74acde4d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 24))\n",
    "for img, _ in test_dataset.take(1):\n",
    "    out1, out2, out3, out4, label = xception.predict(img)\n",
    "    for i in range(3):\n",
    "        plt.subplot(3, 1, i + 1)            \n",
    "        plt.imshow(keras.preprocessing.image.array_to_img(img[i]))    \n",
    "        pred_imglist = []\n",
    "        pred_imglist.append(class_dict[np.argmax(out5[i]) + 1])\n",
    "        plt.title(pred_imglist)\n",
    "        xmin, ymin, xmax, ymax = out1[i]*224, out2[i]*224, out3[i]*224, out4[i]*224\n",
    "        rect = Rectangle((xmin,ymin), (xmax - xmin), (ymax - ymin), fill = False, color = \"r\") \n",
    "        ax = plt.gca()                   \n",
    "        ax.axes.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9aac40-dde6-422c-997d-82e8b22779d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "\n",
    "for img, label in test_dataset:\n",
    "    out1, out2, out3, out4, out5 = label\n",
    "    label = xception.predict(img)\n",
    "    test_labels.append(class_dict[np.argmax(out5) + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625b4ce-38e9-4f9a-95df-fa114b32bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6f80ab-e8ff-46e3-896d-c15e9397e8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels, test_labels, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0f0d3-81b9-4bc5-84eb-b956c8bb5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(true_labels, test_labels, labels = target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fc032-86d8-4a49-98c3-ce872f626d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import InceptionResNetV2 model with pre-trained weights from ImageNet\n",
    "\n",
    "base_inceptionresnetv2 = tf.keras.applications.InceptionResNetV2(weights = \"imagenet\",\n",
    "                                                                 include_top = False,\n",
    "                                                                 input_shape = (224,224,3),\n",
    "                                                                 pooling = 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84c9a6-44a8-41e3-8654-5504cf61c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the base model with fully-connected layers\n",
    "\n",
    "base_inceptionresnetv2.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d591ae41-8250-4f38-8f07-007d7d365e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base model with fully-connected layers\n",
    "inputs = keras.Input(shape = (224,224,3))\n",
    "x = base_inceptionresnetv2(inputs)\n",
    "\n",
    "x1 = keras.layers.Dense(1024, activation = \"relu\")(x)\n",
    "x1 = keras.layers.Dense(512, activation = \"relu\")(x1)\n",
    "out1 = keras.layers.Dense(1, name = \"xmin\")(x1)\n",
    "out2 = keras.layers.Dense(1, name = \"ymin\")(x1)\n",
    "out3 = keras.layers.Dense(1, name = \"xmax\")(x1)\n",
    "out4 = keras.layers.Dense(1, name = \"ymax\")(x1)\n",
    "\n",
    "x2 = keras.layers.Dense(1024, activation = \"relu\")(x)\n",
    "x2 = keras.layers.Dropout(0.5)(x2)\n",
    "x2 = keras.layers.Dense(512, activation = \"relu\")(x2)\n",
    "out_class = keras.layers.Dense(10, activation = \"softmax\", name = \"class\")(x2)\n",
    "\n",
    "out = [out1, out2, out3, out4, out_class]\n",
    "\n",
    "inceptionresnetv2 = keras.models.Model(inputs = inputs, outputs = out)\n",
    "inceptionresnetv2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e3f59-9a2f-4241-abf2-7e2481d00c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "inceptionresnetv2.compile(keras.optimizers.Adam(0.0005),\n",
    "   loss = { \n",
    "      \"xmin\" : \"mse\",\n",
    "      \"ymin\" : \"mse\",\n",
    "      \"xmax\" : \"mse\",\n",
    "      \"ymax\" : \"mse\",\n",
    "      \"class\": \"categorical_crossentropy\" \n",
    "   },\n",
    "   metrics = {\n",
    "      \"xmin\" : \"mae\",\n",
    "      \"ymin\" : \"mae\",\n",
    "      \"xmax\" : \"mae\",\n",
    "      \"ymax\" : \"mae\",\n",
    "      \"class\": \"acc\"\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d99b6-5f1d-4506-8a28-68db787b2360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = ICP_MODEL_PATH\n",
    "CURRENT_META = make_current_meta(\"Inception\", (224, 224))\n",
    "META_PATH  = MODEL_PATH.with_suffix(\".meta.json\")\n",
    "saved_meta = load_meta(ICP_META_PATH)\n",
    "can_reuse = MODEL_PATH.exists() and meta_matches(saved_meta, CURRENT_META) and not FORCE_RETRAIN\n",
    "\n",
    "if can_reuse:\n",
    "    print(f\"Loading Inception from {MODEL_PATH} (compile=True).\")\n",
    "    inceptionresnetv2 = keras.models.load_model(MODEL_PATH, compile=True)\n",
    "    # Load last training curves for plotting\n",
    "    hist_df = load_history_df(\n",
    "        json_path=str(ICP_HISTORY_JSON),\n",
    "        csv_path=str(ICP_HISTORY_CSV),\n",
    "    )\n",
    "else:\n",
    "    print(\"Training Inception...\")\n",
    "    history = train_model(\n",
    "        inceptionresnetv2,\n",
    "        train_dataset, test_dataset,\n",
    "        train_count=train_count, test_count=test_count,\n",
    "        batch_size=batch_size, epochs=200,\n",
    "        callbacks=[lr_reduce],\n",
    "    )\n",
    "    inceptionresnetv2.save(str(MODEL_PATH), include_optimizer=False)\n",
    "    save_meta(CURRENT_META, META_PATH)\n",
    "\n",
    "    # Persist curves next to the model\n",
    "    save_history(\n",
    "        history,\n",
    "        json_path=str(ICP_HISTORY_JSON),\n",
    "        csv_path=str(ICP_HISTORY_CSV),\n",
    "    )\n",
    "    # handy for immediate plotting\n",
    "    hist_df = pd.DataFrame({**history.history, \"epoch\": list(history.epoch)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fc7b8-1509-456b-8b51-b8a2bbcb5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_df(hist_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e2a63-21a5-46bc-b75e-e11aa8170fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = inceptionresnetv2.evaluate(test_dataset, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fe28b8-d0bf-406a-b661-e013c145d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE of xmin value in test set:{}\".format(results['xmin_mae']))\n",
    "print(\"MAE of ymin value in test set:{}\".format(results['ymin_mae']))\n",
    "print(\"MAE of xmax value in test set:{}\".format(results['xmax_mae']))\n",
    "print(\"MAE of ymax value in test set:{}\".format(results['ymax_mae']))\n",
    "print(\"Testing accuracy of predicted label:{}\".format(results['class_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e52e01-be4e-468a-926f-ff4198b68078",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 24))\n",
    "for img, _ in test_dataset.take(1):\n",
    "    out1, out2, out3, out4, label = inceptionresnetv2.predict(img)\n",
    "    for i in range(3):\n",
    "        plt.subplot(3, 1, i + 1)            \n",
    "        plt.imshow(keras.preprocessing.image.array_to_img(img[i]))    \n",
    "        pred_imglist = []\n",
    "        pred_imglist.append(class_dict[np.argmax(out5[i]) + 1])\n",
    "        plt.title(pred_imglist)\n",
    "        xmin, ymin, xmax, ymax = out1[i]*224, out2[i]*224, out3[i]*224, out4[i]*224\n",
    "        rect = Rectangle((xmin,ymin), (xmax - xmin), (ymax - ymin), fill = False, color = \"r\") \n",
    "        ax = plt.gca()                   \n",
    "        ax.axes.add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ccd71d-e8ff-4300-a4ff-eeb168196461",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "\n",
    "for img, label in test_dataset:\n",
    "    out1, out2, out3, out4, out5 = label\n",
    "    label = inceptionresnetv2.predict(img)\n",
    "    test_labels.append(class_dict[np.argmax(out5) + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ad5dc-7d50-4091-8b34-03f2141da22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb80219-a3b4-4280-8e47-3ca318c5d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true_labels, test_labels, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f127dae5-338f-49f9-8de8-852dcb058729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(true_labels, test_labels, labels = target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8fbe4c-4ec1-43a4-a31d-5aebb6723e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
